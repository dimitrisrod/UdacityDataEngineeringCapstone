{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This project recaps the knowledge that I have gained during the Udacity Data Engineering NanoDegree Program.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Imports and installs that are required for the Project.\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType as R, StructField as Fld, DoubleType as Dbl, StringType as Str, IntegerType as Int, DateType as Date, LongType as Long\n",
    "from pyspark.sql.functions import col, split, udf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "toggleable": false,
    "ulab": {
     "buttons": {
      "ulab-button-toggle-f37caf3c": {
       "style": "primary"
      }
     }
    }
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project the US I94 immigration data are explored, enriched and connected with demographics, airport and temperature data for better analysis. The source of data are files and using Pandas and Spark dataframes we are cleaning and transforming, producing the output tables as parquet files.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "#### I94 Immigration Data\n",
    "\n",
    "This data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2027561</th>\n",
       "      <td>4084316.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>209.0</td>\n",
       "      <td>HHW</td>\n",
       "      <td>20566.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HI</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1955.0</td>\n",
       "      <td>07202016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JL</td>\n",
       "      <td>5.658267e+10</td>\n",
       "      <td>00782</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2171295</th>\n",
       "      <td>4422636.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>582.0</td>\n",
       "      <td>MCA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>TX</td>\n",
       "      <td>20568.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>10222016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>*GA</td>\n",
       "      <td>9.436200e+10</td>\n",
       "      <td>XBLNG</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589494</th>\n",
       "      <td>1195600.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>112.0</td>\n",
       "      <td>OGG</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>FL</td>\n",
       "      <td>20571.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1940.0</td>\n",
       "      <td>07052016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>5.578047e+10</td>\n",
       "      <td>00464</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2631158</th>\n",
       "      <td>5291768.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>297.0</td>\n",
       "      <td>LOS</td>\n",
       "      <td>20572.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CA</td>\n",
       "      <td>20581.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>10272016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>QR</td>\n",
       "      <td>9.478970e+10</td>\n",
       "      <td>00739</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3032257</th>\n",
       "      <td>985523.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>CHM</td>\n",
       "      <td>20550.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NY</td>\n",
       "      <td>20553.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>07042016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.232257e+10</td>\n",
       "      <td>LAND</td>\n",
       "      <td>WT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode  \\\n",
       "2027561  4084316.0  2016.0     4.0   209.0   209.0     HHW  20566.0      1.0   \n",
       "2171295  4422636.0  2016.0     4.0   582.0   582.0     MCA  20567.0      1.0   \n",
       "589494   1195600.0  2016.0     4.0   148.0   112.0     OGG  20551.0      1.0   \n",
       "2631158  5291768.0  2016.0     4.0   297.0   297.0     LOS  20572.0      1.0   \n",
       "3032257   985523.0  2016.0     4.0   111.0   111.0     CHM  20550.0      3.0   \n",
       "\n",
       "        i94addr  depdate   ...     entdepu  matflag  biryear   dtaddto gender  \\\n",
       "2027561      HI  20573.0   ...         NaN        M   1955.0  07202016      F   \n",
       "2171295      TX  20568.0   ...         NaN        M   1990.0  10222016      M   \n",
       "589494       FL  20571.0   ...         NaN        M   1940.0  07052016      M   \n",
       "2631158      CA  20581.0   ...         NaN        M   1991.0  10272016      M   \n",
       "3032257      NY  20553.0   ...         NaN        M   1997.0  07042016      F   \n",
       "\n",
       "        insnum airline        admnum  fltno visatype  \n",
       "2027561    NaN      JL  5.658267e+10  00782       WT  \n",
       "2171295    NaN     *GA  9.436200e+10  XBLNG       B2  \n",
       "589494     NaN      LH  5.578047e+10  00464       WT  \n",
       "2631158    NaN      QR  9.478970e+10  00739       B2  \n",
       "3032257    NaN     NaN  4.232257e+10   LAND       WT  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the sample Imigration data.csv and get an idea of how the fact table would look like.\n",
    "\n",
    "filename = './immigration_data_sample.csv'\n",
    "df = pd.read_csv (filename, index_col=[0])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read the i94 SAS data using Spark. \n",
    "\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "df_spark_i94 =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write from Spark dataframe to Parquet files. Then read those files. \n",
    "# This is a demonstration of writing and reading parquet files.\n",
    "\n",
    "df_spark_i94.write.parquet(\"sas_data\")\n",
    "df_spark_i94_f=spark.read.parquet(\"sas_data\")\n",
    "df_spark_i94_f.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>dtadfile</th>\n",
       "      <th>visapost</th>\n",
       "      <th>occup</th>\n",
       "      <th>entdepa</th>\n",
       "      <th>entdepd</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>XXX</td>\n",
       "      <td>20573.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>NaN</td>\n",
       "      <td>U</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>10282016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.897628e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>ATL</td>\n",
       "      <td>20551.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>25.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20130811</td>\n",
       "      <td>SEO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Y</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.736796e+09</td>\n",
       "      <td>00296</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>WAS</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MI</td>\n",
       "      <td>20691.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1961.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OS</td>\n",
       "      <td>6.666432e+08</td>\n",
       "      <td>93</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>NYC</td>\n",
       "      <td>20545.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>20567.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20160401</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2012.0</td>\n",
       "      <td>09302016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AA</td>\n",
       "      <td>9.246846e+10</td>\n",
       "      <td>00199</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    6.0  2016.0     4.0   692.0   692.0     XXX  20573.0      NaN     NaN   \n",
       "1    7.0  2016.0     4.0   254.0   276.0     ATL  20551.0      1.0      AL   \n",
       "2   15.0  2016.0     4.0   101.0   101.0     WAS  20545.0      1.0      MI   \n",
       "3   16.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "4   17.0  2016.0     4.0   101.0   101.0     NYC  20545.0      1.0      MA   \n",
       "\n",
       "   depdate  i94bir  i94visa  count  dtadfile visapost occup entdepa entdepd  \\\n",
       "0      NaN    37.0      2.0    1.0       NaN      NaN   NaN       T     NaN   \n",
       "1      NaN    25.0      3.0    1.0  20130811      SEO   NaN       G     NaN   \n",
       "2  20691.0    55.0      2.0    1.0  20160401      NaN   NaN       T       O   \n",
       "3  20567.0    28.0      2.0    1.0  20160401      NaN   NaN       O       O   \n",
       "4  20567.0     4.0      2.0    1.0  20160401      NaN   NaN       O       O   \n",
       "\n",
       "  entdepu matflag  biryear   dtaddto gender insnum airline        admnum  \\\n",
       "0       U     NaN   1979.0  10282016    NaN    NaN     NaN  1.897628e+09   \n",
       "1       Y     NaN   1991.0       D/S      M    NaN     NaN  3.736796e+09   \n",
       "2     NaN       M   1961.0  09302016      M    NaN      OS  6.666432e+08   \n",
       "3     NaN       M   1988.0  09302016    NaN    NaN      AA  9.246846e+10   \n",
       "4     NaN       M   2012.0  09302016    NaN    NaN      AA  9.246846e+10   \n",
       "\n",
       "   fltno visatype  \n",
       "0    NaN       B2  \n",
       "1  00296       F1  \n",
       "2     93       B2  \n",
       "3  00199       B2  \n",
       "4  00199       B2  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the i94 SAS data using Python dataframe.\n",
    "# Using the Panda dataframes to read the same data, we can notice the big difference in the performance.\n",
    "# Reading the file using Spark is faster than doing the same with Python dataframes.\n",
    "\n",
    "fname = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_i94 = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\")\n",
    "pd.options.display.max_columns = None\n",
    "df_i94.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### World Temperature Data\n",
    "This dataset came from Kaggle. You can read more about it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dt</th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "      <th>City</th>\n",
       "      <th>Country</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1743-11-01</td>\n",
       "      <td>6.068</td>\n",
       "      <td>1.737</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1743-12-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1744-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1744-02-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1744-03-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1744-04-01</td>\n",
       "      <td>5.788</td>\n",
       "      <td>3.624</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1744-05-01</td>\n",
       "      <td>10.644</td>\n",
       "      <td>1.283</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1744-06-01</td>\n",
       "      <td>14.051</td>\n",
       "      <td>1.347</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1744-07-01</td>\n",
       "      <td>16.082</td>\n",
       "      <td>1.396</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1744-08-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1744-09-01</td>\n",
       "      <td>12.781</td>\n",
       "      <td>1.454</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1744-10-01</td>\n",
       "      <td>7.950</td>\n",
       "      <td>1.630</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1744-11-01</td>\n",
       "      <td>4.639</td>\n",
       "      <td>1.302</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1744-12-01</td>\n",
       "      <td>0.122</td>\n",
       "      <td>1.756</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1745-01-01</td>\n",
       "      <td>-1.333</td>\n",
       "      <td>1.642</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1745-02-01</td>\n",
       "      <td>-2.732</td>\n",
       "      <td>1.358</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1745-03-01</td>\n",
       "      <td>0.129</td>\n",
       "      <td>1.088</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1745-04-01</td>\n",
       "      <td>4.042</td>\n",
       "      <td>1.138</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1745-05-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1745-06-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ã…rhus</td>\n",
       "      <td>Denmark</td>\n",
       "      <td>57.05N</td>\n",
       "      <td>10.33E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            dt  AverageTemperature  AverageTemperatureUncertainty   City  \\\n",
       "0   1743-11-01               6.068                          1.737  Ã…rhus   \n",
       "1   1743-12-01                 NaN                            NaN  Ã…rhus   \n",
       "2   1744-01-01                 NaN                            NaN  Ã…rhus   \n",
       "3   1744-02-01                 NaN                            NaN  Ã…rhus   \n",
       "4   1744-03-01                 NaN                            NaN  Ã…rhus   \n",
       "5   1744-04-01               5.788                          3.624  Ã…rhus   \n",
       "6   1744-05-01              10.644                          1.283  Ã…rhus   \n",
       "7   1744-06-01              14.051                          1.347  Ã…rhus   \n",
       "8   1744-07-01              16.082                          1.396  Ã…rhus   \n",
       "9   1744-08-01                 NaN                            NaN  Ã…rhus   \n",
       "10  1744-09-01              12.781                          1.454  Ã…rhus   \n",
       "11  1744-10-01               7.950                          1.630  Ã…rhus   \n",
       "12  1744-11-01               4.639                          1.302  Ã…rhus   \n",
       "13  1744-12-01               0.122                          1.756  Ã…rhus   \n",
       "14  1745-01-01              -1.333                          1.642  Ã…rhus   \n",
       "15  1745-02-01              -2.732                          1.358  Ã…rhus   \n",
       "16  1745-03-01               0.129                          1.088  Ã…rhus   \n",
       "17  1745-04-01               4.042                          1.138  Ã…rhus   \n",
       "18  1745-05-01                 NaN                            NaN  Ã…rhus   \n",
       "19  1745-06-01                 NaN                            NaN  Ã…rhus   \n",
       "\n",
       "    Country Latitude Longitude  \n",
       "0   Denmark   57.05N    10.33E  \n",
       "1   Denmark   57.05N    10.33E  \n",
       "2   Denmark   57.05N    10.33E  \n",
       "3   Denmark   57.05N    10.33E  \n",
       "4   Denmark   57.05N    10.33E  \n",
       "5   Denmark   57.05N    10.33E  \n",
       "6   Denmark   57.05N    10.33E  \n",
       "7   Denmark   57.05N    10.33E  \n",
       "8   Denmark   57.05N    10.33E  \n",
       "9   Denmark   57.05N    10.33E  \n",
       "10  Denmark   57.05N    10.33E  \n",
       "11  Denmark   57.05N    10.33E  \n",
       "12  Denmark   57.05N    10.33E  \n",
       "13  Denmark   57.05N    10.33E  \n",
       "14  Denmark   57.05N    10.33E  \n",
       "15  Denmark   57.05N    10.33E  \n",
       "16  Denmark   57.05N    10.33E  \n",
       "17  Denmark   57.05N    10.33E  \n",
       "18  Denmark   57.05N    10.33E  \n",
       "19  Denmark   57.05N    10.33E  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the World Temperature Data.\n",
    "\n",
    "fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temp = pd.read_csv(fname)\n",
    "df_temp.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### U.S. City Demographic Data\n",
    "This data comes from OpenSoft. You can read more about it here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Peoria</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>33.1</td>\n",
       "      <td>56229.0</td>\n",
       "      <td>62432.0</td>\n",
       "      <td>118661</td>\n",
       "      <td>6634.0</td>\n",
       "      <td>7517.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>IL</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Avondale</td>\n",
       "      <td>Arizona</td>\n",
       "      <td>29.1</td>\n",
       "      <td>38712.0</td>\n",
       "      <td>41971.0</td>\n",
       "      <td>80683</td>\n",
       "      <td>4815.0</td>\n",
       "      <td>8355.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>11592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>West Covina</td>\n",
       "      <td>California</td>\n",
       "      <td>39.8</td>\n",
       "      <td>51629.0</td>\n",
       "      <td>56860.0</td>\n",
       "      <td>108489</td>\n",
       "      <td>3800.0</td>\n",
       "      <td>37038.0</td>\n",
       "      <td>3.56</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>32716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>O'Fallon</td>\n",
       "      <td>Missouri</td>\n",
       "      <td>36.0</td>\n",
       "      <td>41762.0</td>\n",
       "      <td>43270.0</td>\n",
       "      <td>85032</td>\n",
       "      <td>5783.0</td>\n",
       "      <td>3269.0</td>\n",
       "      <td>2.77</td>\n",
       "      <td>MO</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>2583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>High Point</td>\n",
       "      <td>North Carolina</td>\n",
       "      <td>35.5</td>\n",
       "      <td>51751.0</td>\n",
       "      <td>58077.0</td>\n",
       "      <td>109828</td>\n",
       "      <td>5204.0</td>\n",
       "      <td>16315.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>NC</td>\n",
       "      <td>Asian</td>\n",
       "      <td>11060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Folsom</td>\n",
       "      <td>California</td>\n",
       "      <td>40.9</td>\n",
       "      <td>41051.0</td>\n",
       "      <td>35317.0</td>\n",
       "      <td>76368</td>\n",
       "      <td>4187.0</td>\n",
       "      <td>13234.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>CA</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>5822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Folsom</td>\n",
       "      <td>California</td>\n",
       "      <td>40.9</td>\n",
       "      <td>41051.0</td>\n",
       "      <td>35317.0</td>\n",
       "      <td>76368</td>\n",
       "      <td>4187.0</td>\n",
       "      <td>13234.0</td>\n",
       "      <td>2.62</td>\n",
       "      <td>CA</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>34.1</td>\n",
       "      <td>741270.0</td>\n",
       "      <td>826172.0</td>\n",
       "      <td>1567442</td>\n",
       "      <td>61995.0</td>\n",
       "      <td>205339.0</td>\n",
       "      <td>2.61</td>\n",
       "      <td>PA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>122721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Wichita</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>34.6</td>\n",
       "      <td>192354.0</td>\n",
       "      <td>197601.0</td>\n",
       "      <td>389955</td>\n",
       "      <td>23978.0</td>\n",
       "      <td>40270.0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>KS</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>65162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Wichita</td>\n",
       "      <td>Kansas</td>\n",
       "      <td>34.6</td>\n",
       "      <td>192354.0</td>\n",
       "      <td>197601.0</td>\n",
       "      <td>389955</td>\n",
       "      <td>23978.0</td>\n",
       "      <td>40270.0</td>\n",
       "      <td>2.56</td>\n",
       "      <td>KS</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>8791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Fort Myers</td>\n",
       "      <td>Florida</td>\n",
       "      <td>37.3</td>\n",
       "      <td>36850.0</td>\n",
       "      <td>37165.0</td>\n",
       "      <td>74015</td>\n",
       "      <td>4312.0</td>\n",
       "      <td>15365.0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>FL</td>\n",
       "      <td>White</td>\n",
       "      <td>50169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Pittsburgh</td>\n",
       "      <td>Pennsylvania</td>\n",
       "      <td>32.9</td>\n",
       "      <td>149690.0</td>\n",
       "      <td>154695.0</td>\n",
       "      <td>304385</td>\n",
       "      <td>17728.0</td>\n",
       "      <td>28187.0</td>\n",
       "      <td>2.13</td>\n",
       "      <td>PA</td>\n",
       "      <td>White</td>\n",
       "      <td>208863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Laredo</td>\n",
       "      <td>Texas</td>\n",
       "      <td>28.8</td>\n",
       "      <td>124305.0</td>\n",
       "      <td>131484.0</td>\n",
       "      <td>255789</td>\n",
       "      <td>4921.0</td>\n",
       "      <td>68427.0</td>\n",
       "      <td>3.66</td>\n",
       "      <td>TX</td>\n",
       "      <td>American Indian and Alaska Native</td>\n",
       "      <td>1253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Berkeley</td>\n",
       "      <td>California</td>\n",
       "      <td>32.5</td>\n",
       "      <td>60142.0</td>\n",
       "      <td>60829.0</td>\n",
       "      <td>120971</td>\n",
       "      <td>3736.0</td>\n",
       "      <td>25000.0</td>\n",
       "      <td>2.35</td>\n",
       "      <td>CA</td>\n",
       "      <td>Asian</td>\n",
       "      <td>27089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Santa Clara</td>\n",
       "      <td>California</td>\n",
       "      <td>35.2</td>\n",
       "      <td>63278.0</td>\n",
       "      <td>62938.0</td>\n",
       "      <td>126216</td>\n",
       "      <td>4426.0</td>\n",
       "      <td>52281.0</td>\n",
       "      <td>2.75</td>\n",
       "      <td>CA</td>\n",
       "      <td>White</td>\n",
       "      <td>55847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                City           State  Median Age  Male Population  \\\n",
       "0      Silver Spring        Maryland        33.8          40601.0   \n",
       "1             Quincy   Massachusetts        41.0          44129.0   \n",
       "2             Hoover         Alabama        38.5          38040.0   \n",
       "3   Rancho Cucamonga      California        34.5          88127.0   \n",
       "4             Newark      New Jersey        34.6         138040.0   \n",
       "5             Peoria        Illinois        33.1          56229.0   \n",
       "6           Avondale         Arizona        29.1          38712.0   \n",
       "7        West Covina      California        39.8          51629.0   \n",
       "8           O'Fallon        Missouri        36.0          41762.0   \n",
       "9         High Point  North Carolina        35.5          51751.0   \n",
       "10            Folsom      California        40.9          41051.0   \n",
       "11            Folsom      California        40.9          41051.0   \n",
       "12      Philadelphia    Pennsylvania        34.1         741270.0   \n",
       "13           Wichita          Kansas        34.6         192354.0   \n",
       "14           Wichita          Kansas        34.6         192354.0   \n",
       "15        Fort Myers         Florida        37.3          36850.0   \n",
       "16        Pittsburgh    Pennsylvania        32.9         149690.0   \n",
       "17            Laredo           Texas        28.8         124305.0   \n",
       "18          Berkeley      California        32.5          60142.0   \n",
       "19       Santa Clara      California        35.2          63278.0   \n",
       "\n",
       "    Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0             41862.0             82463              1562.0       30908.0   \n",
       "1             49500.0             93629              4147.0       32935.0   \n",
       "2             46799.0             84839              4819.0        8229.0   \n",
       "3             87105.0            175232              5821.0       33878.0   \n",
       "4            143873.0            281913              5829.0       86253.0   \n",
       "5             62432.0            118661              6634.0        7517.0   \n",
       "6             41971.0             80683              4815.0        8355.0   \n",
       "7             56860.0            108489              3800.0       37038.0   \n",
       "8             43270.0             85032              5783.0        3269.0   \n",
       "9             58077.0            109828              5204.0       16315.0   \n",
       "10            35317.0             76368              4187.0       13234.0   \n",
       "11            35317.0             76368              4187.0       13234.0   \n",
       "12           826172.0           1567442             61995.0      205339.0   \n",
       "13           197601.0            389955             23978.0       40270.0   \n",
       "14           197601.0            389955             23978.0       40270.0   \n",
       "15            37165.0             74015              4312.0       15365.0   \n",
       "16           154695.0            304385             17728.0       28187.0   \n",
       "17           131484.0            255789              4921.0       68427.0   \n",
       "18            60829.0            120971              3736.0       25000.0   \n",
       "19            62938.0            126216              4426.0       52281.0   \n",
       "\n",
       "    Average Household Size State Code                               Race  \\\n",
       "0                     2.60         MD                 Hispanic or Latino   \n",
       "1                     2.39         MA                              White   \n",
       "2                     2.58         AL                              Asian   \n",
       "3                     3.18         CA          Black or African-American   \n",
       "4                     2.73         NJ                              White   \n",
       "5                     2.40         IL  American Indian and Alaska Native   \n",
       "6                     3.18         AZ          Black or African-American   \n",
       "7                     3.56         CA                              Asian   \n",
       "8                     2.77         MO                 Hispanic or Latino   \n",
       "9                     2.65         NC                              Asian   \n",
       "10                    2.62         CA                 Hispanic or Latino   \n",
       "11                    2.62         CA  American Indian and Alaska Native   \n",
       "12                    2.61         PA                              Asian   \n",
       "13                    2.56         KS                 Hispanic or Latino   \n",
       "14                    2.56         KS  American Indian and Alaska Native   \n",
       "15                    2.45         FL                              White   \n",
       "16                    2.13         PA                              White   \n",
       "17                    3.66         TX  American Indian and Alaska Native   \n",
       "18                    2.35         CA                              Asian   \n",
       "19                    2.75         CA                              White   \n",
       "\n",
       "     Count  \n",
       "0    25924  \n",
       "1    58723  \n",
       "2     4759  \n",
       "3    24437  \n",
       "4    76402  \n",
       "5     1343  \n",
       "6    11592  \n",
       "7    32716  \n",
       "8     2583  \n",
       "9    11060  \n",
       "10    5822  \n",
       "11     998  \n",
       "12  122721  \n",
       "13   65162  \n",
       "14    8791  \n",
       "15   50169  \n",
       "16  208863  \n",
       "17    1253  \n",
       "18   27089  \n",
       "19   55847  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Demographic data.\n",
    "\n",
    "df_demographics = pd.read_csv(\"./us-cities-demographics.csv\", delimiter=\";\")\n",
    "df_demographics.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Airport Code Table\n",
    "This is a simple table of airport codes and corresponding cities. It comes from here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ident</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>elevation_ft</th>\n",
       "      <th>continent</th>\n",
       "      <th>iso_country</th>\n",
       "      <th>iso_region</th>\n",
       "      <th>municipality</th>\n",
       "      <th>gps_code</th>\n",
       "      <th>iata_code</th>\n",
       "      <th>local_code</th>\n",
       "      <th>coordinates</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00A</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Total Rf Heliport</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-PA</td>\n",
       "      <td>Bensalem</td>\n",
       "      <td>00A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00A</td>\n",
       "      <td>-74.93360137939453, 40.07080078125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00AA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Aero B Ranch Airport</td>\n",
       "      <td>3435.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>Leoti</td>\n",
       "      <td>00AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AA</td>\n",
       "      <td>-101.473911, 38.704022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00AK</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lowell Field</td>\n",
       "      <td>450.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AK</td>\n",
       "      <td>Anchor Point</td>\n",
       "      <td>00AK</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AK</td>\n",
       "      <td>-151.695999146, 59.94919968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00AL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Epps Airpark</td>\n",
       "      <td>820.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AL</td>\n",
       "      <td>Harvest</td>\n",
       "      <td>00AL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AL</td>\n",
       "      <td>-86.77030181884766, 34.86479949951172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00AR</td>\n",
       "      <td>closed</td>\n",
       "      <td>Newport Hospital &amp; Clinic Heliport</td>\n",
       "      <td>237.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AR</td>\n",
       "      <td>Newport</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-91.254898, 35.6087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>00AS</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Fulton Airport</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-OK</td>\n",
       "      <td>Alex</td>\n",
       "      <td>00AS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AS</td>\n",
       "      <td>-97.8180194, 34.9428028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00AZ</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Cordes Airport</td>\n",
       "      <td>3810.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-AZ</td>\n",
       "      <td>Cordes</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00AZ</td>\n",
       "      <td>-112.16500091552734, 34.305599212646484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>00CA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goldstone /Gts/ Airport</td>\n",
       "      <td>3038.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Barstow</td>\n",
       "      <td>00CA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CA</td>\n",
       "      <td>-116.888000488, 35.350498199499995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00CL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Williams Ag Airport</td>\n",
       "      <td>87.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Biggs</td>\n",
       "      <td>00CL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CL</td>\n",
       "      <td>-121.763427, 39.427188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00CN</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kitchen Creek Helibase Heliport</td>\n",
       "      <td>3350.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CA</td>\n",
       "      <td>Pine Valley</td>\n",
       "      <td>00CN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00CN</td>\n",
       "      <td>-116.4597417, 32.7273736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>00CO</td>\n",
       "      <td>closed</td>\n",
       "      <td>Cass Field</td>\n",
       "      <td>4830.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-CO</td>\n",
       "      <td>Briggsdale</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-104.344002, 40.622202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00FA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Grass Patch Airport</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Bushnell</td>\n",
       "      <td>00FA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00FA</td>\n",
       "      <td>-82.21900177001953, 28.64550018310547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00FD</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Ringhaver Heliport</td>\n",
       "      <td>25.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Riverview</td>\n",
       "      <td>00FD</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00FD</td>\n",
       "      <td>-82.34539794921875, 28.846599578857422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00FL</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>River Oak Airport</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-FL</td>\n",
       "      <td>Okeechobee</td>\n",
       "      <td>00FL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00FL</td>\n",
       "      <td>-80.96920013427734, 27.230899810791016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00GA</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Lt World Airport</td>\n",
       "      <td>700.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-GA</td>\n",
       "      <td>Lithonia</td>\n",
       "      <td>00GA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GA</td>\n",
       "      <td>-84.06829833984375, 33.76750183105469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00GE</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Caffrey Heliport</td>\n",
       "      <td>957.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-GA</td>\n",
       "      <td>Hiram</td>\n",
       "      <td>00GE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00GE</td>\n",
       "      <td>-84.73390197753906, 33.88420104980469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00HI</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Kaupulehu Heliport</td>\n",
       "      <td>43.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-HI</td>\n",
       "      <td>Kailua/Kona</td>\n",
       "      <td>00HI</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00HI</td>\n",
       "      <td>-155.980233, 19.832715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00ID</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Delta Shores Airport</td>\n",
       "      <td>2064.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-ID</td>\n",
       "      <td>Clark Fork</td>\n",
       "      <td>00ID</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00ID</td>\n",
       "      <td>-116.21399688720703, 48.145301818847656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00IG</td>\n",
       "      <td>small_airport</td>\n",
       "      <td>Goltl Airport</td>\n",
       "      <td>3359.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-KS</td>\n",
       "      <td>McDonald</td>\n",
       "      <td>00IG</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00IG</td>\n",
       "      <td>-101.395994, 39.724028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00II</td>\n",
       "      <td>heliport</td>\n",
       "      <td>Bailey Generation Station Heliport</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>US</td>\n",
       "      <td>US-IN</td>\n",
       "      <td>Chesterton</td>\n",
       "      <td>00II</td>\n",
       "      <td>NaN</td>\n",
       "      <td>00II</td>\n",
       "      <td>-87.122802734375, 41.644500732421875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ident           type                                name  elevation_ft  \\\n",
       "0    00A       heliport                   Total Rf Heliport          11.0   \n",
       "1   00AA  small_airport                Aero B Ranch Airport        3435.0   \n",
       "2   00AK  small_airport                        Lowell Field         450.0   \n",
       "3   00AL  small_airport                        Epps Airpark         820.0   \n",
       "4   00AR         closed  Newport Hospital & Clinic Heliport         237.0   \n",
       "5   00AS  small_airport                      Fulton Airport        1100.0   \n",
       "6   00AZ  small_airport                      Cordes Airport        3810.0   \n",
       "7   00CA  small_airport             Goldstone /Gts/ Airport        3038.0   \n",
       "8   00CL  small_airport                 Williams Ag Airport          87.0   \n",
       "9   00CN       heliport     Kitchen Creek Helibase Heliport        3350.0   \n",
       "10  00CO         closed                          Cass Field        4830.0   \n",
       "11  00FA  small_airport                 Grass Patch Airport          53.0   \n",
       "12  00FD       heliport                  Ringhaver Heliport          25.0   \n",
       "13  00FL  small_airport                   River Oak Airport          35.0   \n",
       "14  00GA  small_airport                    Lt World Airport         700.0   \n",
       "15  00GE       heliport                    Caffrey Heliport         957.0   \n",
       "16  00HI       heliport                  Kaupulehu Heliport          43.0   \n",
       "17  00ID  small_airport                Delta Shores Airport        2064.0   \n",
       "18  00IG  small_airport                       Goltl Airport        3359.0   \n",
       "19  00II       heliport  Bailey Generation Station Heliport         600.0   \n",
       "\n",
       "   continent iso_country iso_region  municipality gps_code iata_code  \\\n",
       "0        NaN          US      US-PA      Bensalem      00A       NaN   \n",
       "1        NaN          US      US-KS         Leoti     00AA       NaN   \n",
       "2        NaN          US      US-AK  Anchor Point     00AK       NaN   \n",
       "3        NaN          US      US-AL       Harvest     00AL       NaN   \n",
       "4        NaN          US      US-AR       Newport      NaN       NaN   \n",
       "5        NaN          US      US-OK          Alex     00AS       NaN   \n",
       "6        NaN          US      US-AZ        Cordes     00AZ       NaN   \n",
       "7        NaN          US      US-CA       Barstow     00CA       NaN   \n",
       "8        NaN          US      US-CA         Biggs     00CL       NaN   \n",
       "9        NaN          US      US-CA   Pine Valley     00CN       NaN   \n",
       "10       NaN          US      US-CO    Briggsdale      NaN       NaN   \n",
       "11       NaN          US      US-FL      Bushnell     00FA       NaN   \n",
       "12       NaN          US      US-FL     Riverview     00FD       NaN   \n",
       "13       NaN          US      US-FL    Okeechobee     00FL       NaN   \n",
       "14       NaN          US      US-GA      Lithonia     00GA       NaN   \n",
       "15       NaN          US      US-GA         Hiram     00GE       NaN   \n",
       "16       NaN          US      US-HI   Kailua/Kona     00HI       NaN   \n",
       "17       NaN          US      US-ID    Clark Fork     00ID       NaN   \n",
       "18       NaN          US      US-KS      McDonald     00IG       NaN   \n",
       "19       NaN          US      US-IN    Chesterton     00II       NaN   \n",
       "\n",
       "   local_code                              coordinates  \n",
       "0         00A       -74.93360137939453, 40.07080078125  \n",
       "1        00AA                   -101.473911, 38.704022  \n",
       "2        00AK              -151.695999146, 59.94919968  \n",
       "3        00AL    -86.77030181884766, 34.86479949951172  \n",
       "4         NaN                      -91.254898, 35.6087  \n",
       "5        00AS                  -97.8180194, 34.9428028  \n",
       "6        00AZ  -112.16500091552734, 34.305599212646484  \n",
       "7        00CA       -116.888000488, 35.350498199499995  \n",
       "8        00CL                   -121.763427, 39.427188  \n",
       "9        00CN                 -116.4597417, 32.7273736  \n",
       "10        NaN                   -104.344002, 40.622202  \n",
       "11       00FA    -82.21900177001953, 28.64550018310547  \n",
       "12       00FD   -82.34539794921875, 28.846599578857422  \n",
       "13       00FL   -80.96920013427734, 27.230899810791016  \n",
       "14       00GA    -84.06829833984375, 33.76750183105469  \n",
       "15       00GE    -84.73390197753906, 33.88420104980469  \n",
       "16       00HI                   -155.980233, 19.832715  \n",
       "17       00ID  -116.21399688720703, 48.145301818847656  \n",
       "18       00IG                   -101.395994, 39.724028  \n",
       "19       00II     -87.122802734375, 41.644500732421875  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Airport Code table data.\n",
    "\n",
    "df_airport_codes = pd.read_csv(\"./airport-codes_csv.csv\")\n",
    "df_airport_codes.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cicid=6.0, i94yr=2016.0, i94mon=4.0, i94cit=692.0, i94res=692.0, i94port='XXX', arrdate=20573.0, i94mode=None, i94addr=None, depdate=None, i94bir=37.0, i94visa=2.0, count=1.0, dtadfile=None, visapost=None, occup=None, entdepa='T', entdepd=None, entdepu='U', matflag=None, biryear=1979.0, dtaddto='10282016', gender=None, insnum=None, airline=None, admnum=1897628485.0, fltno=None, visatype='B2'),\n",
       " Row(cicid=7.0, i94yr=2016.0, i94mon=4.0, i94cit=254.0, i94res=276.0, i94port='ATL', arrdate=20551.0, i94mode=1.0, i94addr='AL', depdate=None, i94bir=25.0, i94visa=3.0, count=1.0, dtadfile='20130811', visapost='SEO', occup=None, entdepa='G', entdepd=None, entdepu='Y', matflag=None, biryear=1991.0, dtaddto='D/S', gender='M', insnum=None, airline=None, admnum=3736796330.0, fltno='00296', visatype='F1'),\n",
       " Row(cicid=15.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='WAS', arrdate=20545.0, i94mode=1.0, i94addr='MI', depdate=20691.0, i94bir=55.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='T', entdepd='O', entdepu=None, matflag='M', biryear=1961.0, dtaddto='09302016', gender='M', insnum=None, airline='OS', admnum=666643185.0, fltno='93', visatype='B2'),\n",
       " Row(cicid=16.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=28.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=1988.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468461330.0, fltno='00199', visatype='B2'),\n",
       " Row(cicid=17.0, i94yr=2016.0, i94mon=4.0, i94cit=101.0, i94res=101.0, i94port='NYC', arrdate=20545.0, i94mode=1.0, i94addr='MA', depdate=20567.0, i94bir=4.0, i94visa=2.0, count=1.0, dtadfile='20160401', visapost=None, occup=None, entdepa='O', entdepd='O', entdepu=None, matflag='M', biryear=2012.0, dtaddto='09302016', gender=None, insnum=None, airline='AA', admnum=92468463130.0, fltno='00199', visatype='B2')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore the Spark dataframe that contains the I94 data.\n",
    "\n",
    "df_spark_i94.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>depdate</th>\n",
       "      <th>i94bir</th>\n",
       "      <th>i94visa</th>\n",
       "      <th>count</th>\n",
       "      <th>biryear</th>\n",
       "      <th>admnum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3096313.0</td>\n",
       "      <td>3096313.0</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3.096074e+06</td>\n",
       "      <td>2.953856e+06</td>\n",
       "      <td>3.095511e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "      <td>3096313.0</td>\n",
       "      <td>3.095511e+06</td>\n",
       "      <td>3.096313e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.078652e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.049069e+02</td>\n",
       "      <td>3.032838e+02</td>\n",
       "      <td>2.055985e+04</td>\n",
       "      <td>1.073690e+00</td>\n",
       "      <td>2.057395e+04</td>\n",
       "      <td>4.176761e+01</td>\n",
       "      <td>1.845393e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.974232e+03</td>\n",
       "      <td>7.082885e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.763278e+06</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.100269e+02</td>\n",
       "      <td>2.085832e+02</td>\n",
       "      <td>8.777339e+00</td>\n",
       "      <td>5.158963e-01</td>\n",
       "      <td>2.935697e+01</td>\n",
       "      <td>1.742026e+01</td>\n",
       "      <td>3.983910e-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.742026e+01</td>\n",
       "      <td>2.215442e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.000000e+00</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>1.010000e+02</td>\n",
       "      <td>2.054500e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.517600e+04</td>\n",
       "      <td>-3.000000e+00</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.902000e+03</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.577790e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.350000e+02</td>\n",
       "      <td>1.310000e+02</td>\n",
       "      <td>2.055200e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.056100e+04</td>\n",
       "      <td>3.000000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.962000e+03</td>\n",
       "      <td>5.603523e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.103507e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>2.130000e+02</td>\n",
       "      <td>2.056000e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.057000e+04</td>\n",
       "      <td>4.100000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.975000e+03</td>\n",
       "      <td>5.936094e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.654341e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.120000e+02</td>\n",
       "      <td>5.040000e+02</td>\n",
       "      <td>2.056700e+04</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.057900e+04</td>\n",
       "      <td>5.400000e+01</td>\n",
       "      <td>2.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.986000e+03</td>\n",
       "      <td>9.350987e+10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6.102785e+06</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.990000e+02</td>\n",
       "      <td>7.600000e+02</td>\n",
       "      <td>2.057400e+04</td>\n",
       "      <td>9.000000e+00</td>\n",
       "      <td>4.542700e+04</td>\n",
       "      <td>1.140000e+02</td>\n",
       "      <td>3.000000e+00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.019000e+03</td>\n",
       "      <td>9.991557e+10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cicid      i94yr     i94mon        i94cit        i94res  \\\n",
       "count  3.096313e+06  3096313.0  3096313.0  3.096313e+06  3.096313e+06   \n",
       "mean   3.078652e+06     2016.0        4.0  3.049069e+02  3.032838e+02   \n",
       "std    1.763278e+06        0.0        0.0  2.100269e+02  2.085832e+02   \n",
       "min    6.000000e+00     2016.0        4.0  1.010000e+02  1.010000e+02   \n",
       "25%    1.577790e+06     2016.0        4.0  1.350000e+02  1.310000e+02   \n",
       "50%    3.103507e+06     2016.0        4.0  2.130000e+02  2.130000e+02   \n",
       "75%    4.654341e+06     2016.0        4.0  5.120000e+02  5.040000e+02   \n",
       "max    6.102785e+06     2016.0        4.0  9.990000e+02  7.600000e+02   \n",
       "\n",
       "            arrdate       i94mode       depdate        i94bir       i94visa  \\\n",
       "count  3.096313e+06  3.096074e+06  2.953856e+06  3.095511e+06  3.096313e+06   \n",
       "mean   2.055985e+04  1.073690e+00  2.057395e+04  4.176761e+01  1.845393e+00   \n",
       "std    8.777339e+00  5.158963e-01  2.935697e+01  1.742026e+01  3.983910e-01   \n",
       "min    2.054500e+04  1.000000e+00  1.517600e+04 -3.000000e+00  1.000000e+00   \n",
       "25%    2.055200e+04  1.000000e+00  2.056100e+04  3.000000e+01  2.000000e+00   \n",
       "50%    2.056000e+04  1.000000e+00  2.057000e+04  4.100000e+01  2.000000e+00   \n",
       "75%    2.056700e+04  1.000000e+00  2.057900e+04  5.400000e+01  2.000000e+00   \n",
       "max    2.057400e+04  9.000000e+00  4.542700e+04  1.140000e+02  3.000000e+00   \n",
       "\n",
       "           count       biryear        admnum  \n",
       "count  3096313.0  3.095511e+06  3.096313e+06  \n",
       "mean         1.0  1.974232e+03  7.082885e+10  \n",
       "std          0.0  1.742026e+01  2.215442e+10  \n",
       "min          1.0  1.902000e+03  0.000000e+00  \n",
       "25%          1.0  1.962000e+03  5.603523e+10  \n",
       "50%          1.0  1.975000e+03  5.936094e+10  \n",
       "75%          1.0  1.986000e+03  9.350987e+10  \n",
       "max          1.0  2.019000e+03  9.991557e+10  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the i94 data. (Pandas dataframe)\n",
    "df_i94.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AverageTemperature</th>\n",
       "      <th>AverageTemperatureUncertainty</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>8.235082e+06</td>\n",
       "      <td>8.235082e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.672743e+01</td>\n",
       "      <td>1.028575e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.035344e+01</td>\n",
       "      <td>1.129733e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.270400e+01</td>\n",
       "      <td>3.400000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.029900e+01</td>\n",
       "      <td>3.370000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.883100e+01</td>\n",
       "      <td>5.910000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.521000e+01</td>\n",
       "      <td>1.349000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.965100e+01</td>\n",
       "      <td>1.539600e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AverageTemperature  AverageTemperatureUncertainty\n",
       "count        8.235082e+06                   8.235082e+06\n",
       "mean         1.672743e+01                   1.028575e+00\n",
       "std          1.035344e+01                   1.129733e+00\n",
       "min         -4.270400e+01                   3.400000e-02\n",
       "25%          1.029900e+01                   3.370000e-01\n",
       "50%          1.883100e+01                   5.910000e-01\n",
       "75%          2.521000e+01                   1.349000e+00\n",
       "max          3.965100e+01                   1.539600e+01"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the temperature data.\n",
    "df_temp.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>elevation_ft</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48069.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1240.789677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1602.363459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>205.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>718.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1497.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>22000.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       elevation_ft\n",
       "count  48069.000000\n",
       "mean    1240.789677\n",
       "std     1602.363459\n",
       "min    -1266.000000\n",
       "25%      205.000000\n",
       "50%      718.000000\n",
       "75%     1497.000000\n",
       "max    22000.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the airport data.\n",
    "df_airport_codes.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2891.000000</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.888000e+03</td>\n",
       "      <td>2.891000e+03</td>\n",
       "      <td>2878.000000</td>\n",
       "      <td>2.878000e+03</td>\n",
       "      <td>2875.000000</td>\n",
       "      <td>2.891000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>35.494881</td>\n",
       "      <td>9.732843e+04</td>\n",
       "      <td>1.017696e+05</td>\n",
       "      <td>1.989668e+05</td>\n",
       "      <td>9367.832523</td>\n",
       "      <td>4.065360e+04</td>\n",
       "      <td>2.742543</td>\n",
       "      <td>4.896377e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4.401617</td>\n",
       "      <td>2.162999e+05</td>\n",
       "      <td>2.315646e+05</td>\n",
       "      <td>4.475559e+05</td>\n",
       "      <td>13211.219924</td>\n",
       "      <td>1.557491e+05</td>\n",
       "      <td>0.433291</td>\n",
       "      <td>1.443856e+05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>22.900000</td>\n",
       "      <td>2.928100e+04</td>\n",
       "      <td>2.734800e+04</td>\n",
       "      <td>6.321500e+04</td>\n",
       "      <td>416.000000</td>\n",
       "      <td>8.610000e+02</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>9.800000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>32.800000</td>\n",
       "      <td>3.928900e+04</td>\n",
       "      <td>4.122700e+04</td>\n",
       "      <td>8.042900e+04</td>\n",
       "      <td>3739.000000</td>\n",
       "      <td>9.224000e+03</td>\n",
       "      <td>2.430000</td>\n",
       "      <td>3.435000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>35.300000</td>\n",
       "      <td>5.234100e+04</td>\n",
       "      <td>5.380900e+04</td>\n",
       "      <td>1.067820e+05</td>\n",
       "      <td>5397.000000</td>\n",
       "      <td>1.882200e+04</td>\n",
       "      <td>2.650000</td>\n",
       "      <td>1.378000e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>38.000000</td>\n",
       "      <td>8.664175e+04</td>\n",
       "      <td>8.960400e+04</td>\n",
       "      <td>1.752320e+05</td>\n",
       "      <td>9368.000000</td>\n",
       "      <td>3.397175e+04</td>\n",
       "      <td>2.950000</td>\n",
       "      <td>5.444700e+04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>70.500000</td>\n",
       "      <td>4.081698e+06</td>\n",
       "      <td>4.468707e+06</td>\n",
       "      <td>8.550405e+06</td>\n",
       "      <td>156961.000000</td>\n",
       "      <td>3.212500e+06</td>\n",
       "      <td>4.980000</td>\n",
       "      <td>3.835726e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Median Age  Male Population  Female Population  Total Population  \\\n",
       "count  2891.000000     2.888000e+03       2.888000e+03      2.891000e+03   \n",
       "mean     35.494881     9.732843e+04       1.017696e+05      1.989668e+05   \n",
       "std       4.401617     2.162999e+05       2.315646e+05      4.475559e+05   \n",
       "min      22.900000     2.928100e+04       2.734800e+04      6.321500e+04   \n",
       "25%      32.800000     3.928900e+04       4.122700e+04      8.042900e+04   \n",
       "50%      35.300000     5.234100e+04       5.380900e+04      1.067820e+05   \n",
       "75%      38.000000     8.664175e+04       8.960400e+04      1.752320e+05   \n",
       "max      70.500000     4.081698e+06       4.468707e+06      8.550405e+06   \n",
       "\n",
       "       Number of Veterans  Foreign-born  Average Household Size         Count  \n",
       "count         2878.000000  2.878000e+03             2875.000000  2.891000e+03  \n",
       "mean          9367.832523  4.065360e+04                2.742543  4.896377e+04  \n",
       "std          13211.219924  1.557491e+05                0.433291  1.443856e+05  \n",
       "min            416.000000  8.610000e+02                2.000000  9.800000e+01  \n",
       "25%           3739.000000  9.224000e+03                2.430000  3.435000e+03  \n",
       "50%           5397.000000  1.882200e+04                2.650000  1.378000e+04  \n",
       "75%           9368.000000  3.397175e+04                2.950000  5.444700e+04  \n",
       "max         156961.000000  3.212500e+06                4.980000  3.835726e+06  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exploring the demographics data.\n",
    "df_demographics.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Cleaning Steps\n",
    "I have created reference files using data from the I94_SAS_Labels_Descriptions file.\n",
    "Cleaning and using the dataframes to replace column data in the I94 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_countries():\n",
    "    # Cleaning and loading the Countries data into a dataframe\n",
    "    fname = './reference/i94cntyl.txt'\n",
    "    df_countries = pd.read_csv(fname, sep = \" =  \",names = [\"country_code\", \"country\"], header=None, skipinitialspace= True, engine='python')\n",
    "    df_countries[\"country\"] = df_countries[\"country\"].astype(str).str.replace(\"'\", \"\")\n",
    "    df_countries.loc[df_countries[\"country\"].str.contains(\"INVALID*\"), \"country\"] = \"Other\"\n",
    "    df_countries.loc[df_countries[\"country\"].str.contains(\"No Country*\"), \"country\"] = \"Other\"\n",
    "    df_countries.loc[df_countries[\"country\"].str.contains(\"Collapsed*\"), \"country\"] = \"Other\"\n",
    "    df_countries.loc[df_countries[\"country\"].str.contains(\"Other*\"), \"country\"] = \"Other\"\n",
    "\n",
    "    # Create a spark Df from the Pandas Df\n",
    "    df_spark_countries = spark.createDataFrame(df_countries)\n",
    "    \n",
    "    # perform data quality checks\n",
    "    df_spark_countries.show(5, truncate=False)\n",
    "    df_spark_countries.printSchema()\n",
    "    df_spark_countries.count()\n",
    "    \n",
    "    return df_spark_countries\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_states():\n",
    "\n",
    "    # Cleaning and loading the States data into a dataframe\n",
    "\n",
    "    fname = './reference/i94addrl.txt'\n",
    "    df_states = pd.read_csv(fname, sep = \"=\",names = [\"state_code\", \"state\"], header=None, skipinitialspace= True, engine='python')\n",
    "    df_states[\"state\"] = df_states[\"state\"].astype(str).str.replace(\"'\", \"\")\n",
    "    df_states.iloc[ : , 0 ] = df_states.iloc[ : , 0].str.replace(\"'\", \"\").str.replace(\"\\t\", \"\")\n",
    "\n",
    "    # Create a spark Df from the Pandas Df\n",
    "    df_spark_states = spark.createDataFrame(df_states)\n",
    "    \n",
    "    # perform data quality checks\n",
    "    df_spark_states.show(5, truncate=False)\n",
    "    df_spark_states.printSchema()\n",
    "    df_spark_states.count()\n",
    "    \n",
    "    return df_spark_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_ports():\n",
    "\n",
    "    # Cleaning and loading the Port data into a dataframe\n",
    "\n",
    "    fname = './reference/i94prtl.txt'\n",
    "    df_ports = pd.read_csv(fname, sep = \"\t=\t\",names = [\"port_code\", \"city\"], header=None, skipinitialspace= True, engine='python')\n",
    "    df_ports[\"port_code\"] = df_ports[\"port_code\"].astype(str).str.replace(\"'\", \"\")\n",
    "    df_ports[\"city\"] = df_ports[\"city\"].astype(str).str.replace(\"'\", \"\")\n",
    "    df_new = df_ports[\"city\"].str.split(\", \", n = 1, expand = True) \n",
    "\n",
    "    # making separate state column from new data frame \n",
    "    df_ports[\"state\"]= df_new[1].str.strip()\n",
    "\n",
    "    # replacing the value of city column from new data frame \n",
    "    df_ports[\"city\"]= df_new[0] \n",
    "\n",
    "    # Create a spark Df from the Pandas Df\n",
    "    df_spark_ports = spark.createDataFrame(df_ports)\n",
    "    \n",
    "    # perform data quality checks\n",
    "    df_spark_ports.show(5, truncate=False)\n",
    "    df_spark_ports.printSchema()\n",
    "    df_spark_ports.count()\n",
    "    \n",
    "    return df_spark_ports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_modes():\n",
    "\n",
    "    # Cleaning and loading the Mode data into a dataframe.\n",
    "\n",
    "    fname = './reference/i94model.txt'\n",
    "    df_modes = pd.read_csv(fname, sep = \" = \",names = [\"mode_code\", \"mode\"], header=None, skipinitialspace= True, engine='python')\n",
    "    df_modes[\"mode\"] = df_modes[\"mode\"].astype(str).str.replace(\"'\", \"\")\n",
    "\n",
    "    # Create a spark Df from the Pandas Df\n",
    "    df_spark_modes = spark.createDataFrame(df_modes)\n",
    "    \n",
    "    # perform data quality checks\n",
    "    df_spark_modes.show(5, truncate=False)\n",
    "    df_spark_modes.printSchema()\n",
    "    df_spark_modes.count()\n",
    "    \n",
    "    return df_spark_modes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_visas():\n",
    "    \n",
    "    # Cleaning visas and loading into a dataframe.\n",
    "    fname = './reference/i94visa.txt'\n",
    "    df_visas = pd.read_csv(fname, sep = \" = \",names = [\"visa_code\", \"visa\"], header=None, skipinitialspace= True, engine='python')\n",
    "    df_visas[\"visa\"] = df_visas[\"visa\"].astype(str).str.replace(\"'\", \"\")\n",
    "\n",
    "    # Create a spark Df from the Pandas Df\n",
    "    df_spark_visas = spark.createDataFrame(df_visas)\n",
    "    \n",
    "    # perform data quality checks\n",
    "    df_spark_visas.show(5, truncate=False)\n",
    "    df_spark_visas.printSchema()\n",
    "    df_spark_visas.count()\n",
    "    \n",
    "    return df_spark_visas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_temperatures():\n",
    "\n",
    "    # Filtering the Temperature data to only the United States records. \n",
    "    # Then drop the unnecessary Country column.\n",
    "    # Clean from empty values.\n",
    "\n",
    "    df_us_temp = df_temp[df_temp[\"Country\"] == \"United States\"]\n",
    "    df_us_temp = df_us_temp.drop('Country', 1)\n",
    "    df_us_temp.dropna(inplace=True)\n",
    "    df_us_temp.head()\n",
    "    \n",
    "    return df_us_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------------------------------------------------+\n",
      "|country_code|country                                                  |\n",
      "+------------+---------------------------------------------------------+\n",
      "|582         |MEXICO Air Sea, and Not Reported (I-94, no land arrivals)|\n",
      "|236         |AFGHANISTAN                                              |\n",
      "|101         |ALBANIA                                                  |\n",
      "|316         |ALGERIA                                                  |\n",
      "|102         |ANDORRA                                                  |\n",
      "+------------+---------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- country_code: long (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      "\n",
      "+----------+----------+\n",
      "|state_code|state     |\n",
      "+----------+----------+\n",
      "|AL        |ALABAMA   |\n",
      "|AK        |ALASKA    |\n",
      "|AZ        |ARIZONA   |\n",
      "|AR        |ARKANSAS  |\n",
      "|CA        |CALIFORNIA|\n",
      "+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+------------------------+-----+\n",
      "|port_code|city                    |state|\n",
      "+---------+------------------------+-----+\n",
      "|ALC      |ALCAN                   |AK   |\n",
      "|ANC      |ANCHORAGE               |AK   |\n",
      "|BAR      |BAKER AAF - BAKER ISLAND|AK   |\n",
      "|DAC      |DALTONS CACHE           |AK   |\n",
      "|PIZ      |DEW STATION PT LAY DEW  |AK   |\n",
      "+---------+------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- port_code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      "\n",
      "+---------+------------+\n",
      "|mode_code|mode        |\n",
      "+---------+------------+\n",
      "|1        |Air         |\n",
      "|2        |Sea         |\n",
      "|3        |Land        |\n",
      "|9        |Not reported|\n",
      "+---------+------------+\n",
      "\n",
      "root\n",
      " |-- mode_code: long (nullable = true)\n",
      " |-- mode: string (nullable = true)\n",
      "\n",
      "+---------+--------+\n",
      "|visa_code|visa    |\n",
      "+---------+--------+\n",
      "|1        |Business|\n",
      "|2        |Pleasure|\n",
      "|3        |Student |\n",
      "+---------+--------+\n",
      "\n",
      "root\n",
      " |-- visa_code: long (nullable = true)\n",
      " |-- visa: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the functions that clean all the reference tables.\n",
    "\n",
    "df_spark_countries = clean_countries()\n",
    "df_spark_states = clean_states()\n",
    "df_spark_ports = clean_ports()\n",
    "df_spark_modes = clean_modes()\n",
    "df_spark_visas = clean_visas()\n",
    "df_us_temp = clean_temperatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The conceptual data model that I designed, contains the Immigrations fact table with direct relations to the airport, countries and demographics which are the dimensions tables. For further analysis of the temperatures data there is a direct relationship to the demographics table.\n",
    "\n",
    "I have chosen the star schema, because the queries will focus on analysing the immigration data. Some examples of analytics queries on the current data model would be: \n",
    "From which countries do immigrants mostly come from?\n",
    "What are the types of airports that they use mostly?\n",
    "What is the total population of the state they arrive to?\n",
    "What is the average temperature for the most visited locations?\n",
    "\n",
    "\n",
    "![alt text](dmodel.png \"Data Model\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_temperature():\n",
    "\n",
    "    # Create the Spark Dataframe schema that will host the temperature data.\n",
    "    # Then create a Spark df from the existing pandas df.\n",
    "\n",
    "    tempSchema = R([\n",
    "                            Fld(\"date\",Str()),\n",
    "                            Fld(\"average_temperature\",Dbl()),\n",
    "                            Fld(\"average_temperature_uncertainty\",Dbl()),\n",
    "                            Fld(\"city\",Str()),\n",
    "                            Fld(\"latitude\",Str()),\n",
    "                            Fld(\"longitude\",Str())    \n",
    "                            ])\n",
    "    df_us_temperature = spark.createDataFrame(df_us_temp,tempSchema)\n",
    "    df_us_temperature.printSchema()\n",
    "    \n",
    "    return df_us_temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_airport_codes():\n",
    "\n",
    "    # Clean the airport codes from empty iata_codes, because this is the field that will be used as PK in the dimension table.\n",
    "    # Filter the records only to the US ones and drop the iso_country column.\n",
    "\n",
    "    df_airport_codes.dropna(subset=['iata_code'], inplace=True)\n",
    "    df_airport_codes_flt= df_airport_codes[df_airport_codes[\"iso_country\"] == \"US\"]\n",
    "    df_airport_codes_flt = df_airport_codes_flt.drop('iso_country', 1)\n",
    "\n",
    "\n",
    "    # Create a spark Df from the Pandas Df.\n",
    "\n",
    "    airportSchema = R([\n",
    "                        Fld(\"airport_id\",Str()),\n",
    "                        Fld(\"type\",Str()),\n",
    "                        Fld(\"name\",Str()),\n",
    "                        Fld(\"elevation_ft\",Str()),\n",
    "                        Fld(\"continent\",Str()),\n",
    "                        Fld(\"iso_region\",Str()),\n",
    "                        Fld(\"municipality\",Str()),\n",
    "                        Fld(\"gps_code\",Str()),\n",
    "                        Fld(\"iata_code\",Str()),\n",
    "                        Fld(\"local_code\",Str()),\n",
    "                        Fld(\"coordinates\",Str())\n",
    "                        ])\n",
    "    df_spark_airport_codes = spark.createDataFrame(df_airport_codes_flt,schema=airportSchema)\n",
    "\n",
    "    # Split the iso_region field and keep the State as a new one.\n",
    "    # Split the coordinates field and create latitude and longitude.\n",
    "    # Drop coordinates, iso_region and continent.\n",
    "\n",
    "    df_spark_airport_codes_flt = df_spark_airport_codes\\\n",
    "                                     .withColumn(\"state\", split(col(\"iso_region\"), \"-\")[1])\\\n",
    "                                     .withColumn(\"latitude\", split(col(\"coordinates\"), \",\")[0].cast(Dbl()))\\\n",
    "                                     .withColumn(\"longitude\", split(col(\"coordinates\"), \",\")[1].cast(Dbl()))\\\n",
    "                                     .drop(\"coordinates\")\\\n",
    "                                     .drop(\"iso_region\")\\\n",
    "                                     .drop(\"continent\")\n",
    "    \n",
    "    # Clean dataframe from possible duplicates on the iata_code.\n",
    "    df_spark_airport_codes_clean = df_spark_airport_codes_flt.dropDuplicates([\"iata_code\"])\n",
    "    df_spark_airport_codes_clean.printSchema()\n",
    "    \n",
    "    return df_spark_airport_codes_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_demographics():\n",
    "\n",
    "    # Create a schema to host the demographics data.\n",
    "\n",
    "    demogrSchema = R([\n",
    "                                Fld(\"city\",Str()),\n",
    "                                Fld(\"state\",Str()),\n",
    "                                Fld(\"median_age\",Dbl()),\n",
    "                                Fld(\"male_population\",Int()),\n",
    "                                Fld(\"female_population\",Int()),\n",
    "                                Fld(\"total_population\",Int()),\n",
    "                                Fld(\"number_of_veterans\",Int()),\n",
    "                                Fld(\"number_of_foreign_born\",Int()),\n",
    "                                Fld(\"average_household_size\",Dbl()),\n",
    "                                Fld(\"state_code\",Str()),\n",
    "                                Fld(\"race\",Str()),\n",
    "                                Fld(\"count\",Int()) \n",
    "                                ])\n",
    "\n",
    "    #Load the csv data into a Spark df.\n",
    "    df_spark_demographics = spark.read.csv(\"./us-cities-demographics.csv\", header='true', sep=\";\", schema=demogrSchema)\n",
    "\n",
    "    # Filter the demographics df for the empty state records.\n",
    "    # Additionally, filter the duplicates on the combination of state,city,race.\n",
    "    df_demographics_cl = df_spark_demographics.filter(df_spark_demographics.state.isNotNull())\\\n",
    "                               .dropDuplicates(subset=['state', 'city', 'race'])\n",
    "\n",
    "    df_demographics_cl.printSchema()\n",
    "    \n",
    "    return df_demographics_cl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- average_temperature: double (nullable = true)\n",
      " |-- average_temperature_uncertainty: double (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- airport_id: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- number_of_veterans: integer (nullable = true)\n",
      " |-- number_of_foreign_born: integer (nullable = true)\n",
      " |-- average_household_size: double (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- race: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the functions that creates the dimension tables/dataframes\n",
    "\n",
    "df_us_temperature = create_temperature() \n",
    "df_spark_airport_codes_clean = create_airport_codes()\n",
    "df_demographics_cl = create_demographics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_immigrations():\n",
    "\n",
    "    # Create the function that will get the date from the SAS format.\n",
    "    get_date = udf(lambda x: (dt.datetime(1960, 1, 1).date() + dt.timedelta(x)).isoformat() if x else None)\n",
    "\n",
    "    # Transform the date format for the arrdate and depdate.\n",
    "    df_spark_i94_new = df_spark_i94.withColumn(\"arrdate\", get_date(df_spark_i94.arrdate)).withColumn(\"depdate\", get_date(df_spark_i94.depdate))\n",
    "\n",
    "\n",
    "    df_spark_i94_new.createOrReplaceTempView(\"immigrations\")\n",
    "    df_spark_states.createOrReplaceTempView(\"states\")\n",
    "    df_spark_visas.createOrReplaceTempView(\"visas\")\n",
    "    df_spark_modes.createOrReplaceTempView(\"modes\") \n",
    "\n",
    "    # Get the modes and visas from the reference mappings, set missing states to 99.\n",
    "    df_immigrations = spark.sql(\"\"\"\n",
    "                                            select i.cicid,\n",
    "                                                    i.i94yr as year,\n",
    "                                                    i.i94mon as month,\n",
    "                                                    i.i94cit as birth_country,\n",
    "                                                    i.i94res as residence_country,\n",
    "                                                    i.i94port as port,\n",
    "                                                    i.arrdate as arrival_date,\n",
    "                                                    coalesce(m.mode, 'Not reported') as arrival_mode,\n",
    "                                                    coalesce(c.state_code, '99') as us_state,\n",
    "                                                    i.depdate as departure_date,\n",
    "                                                    i.i94bir as age,\n",
    "                                                    coalesce(v.visa, 'Other') as visa_type_code,\n",
    "                                                    i.dtadfile as date_added,\n",
    "                                                    i.visapost as visa_issued_department,\n",
    "                                                    i.occup as occupation,\n",
    "                                                    i.entdepa as arrival_flag,\n",
    "                                                    i.entdepd as departure_flag,\n",
    "                                                    i.entdepu as update_flag,\n",
    "                                                    i.matflag as match_arrival_dep_flag,\n",
    "                                                    i.biryear as birth_year,\n",
    "                                                    i.dtaddto as allowed_date,\n",
    "                                                    i.insnum as ins_number,\n",
    "                                                    i.airline as airline,\n",
    "                                                    i.admnum as admission_number,\n",
    "                                                    i.fltno as flight_number,\n",
    "                                                    i.visatype as visa_type\n",
    "                                                from immigrations i left join states c on i.i94addr=c.state_code\n",
    "                                                    left join visas v on i.i94visa=v.visa_code\n",
    "                                                    left join modes m on i.i94mode=m.mode_code\n",
    "                                            \"\"\")\n",
    "\n",
    "    # Drop unnecessary flags from the dataframe.\n",
    "    df_immigrations = df_immigrations.drop(\"arrival_flag\",\"departure_flag\",\"update_flag\",\"match_arrival_dep_flag\")\n",
    "    df_immigrations.printSchema()\n",
    "    \n",
    "    return df_immigrations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- month: double (nullable = true)\n",
      " |-- birth_country: double (nullable = true)\n",
      " |-- residence_country: double (nullable = true)\n",
      " |-- port: string (nullable = true)\n",
      " |-- arrival_date: string (nullable = true)\n",
      " |-- arrival_mode: string (nullable = false)\n",
      " |-- us_state: string (nullable = false)\n",
      " |-- departure_date: string (nullable = true)\n",
      " |-- age: double (nullable = true)\n",
      " |-- visa_type_code: string (nullable = false)\n",
      " |-- date_added: string (nullable = true)\n",
      " |-- visa_issued_department: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- birth_year: double (nullable = true)\n",
      " |-- allowed_date: string (nullable = true)\n",
      " |-- ins_number: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admission_number: double (nullable = true)\n",
      " |-- flight_number: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the function that creates the fact table.\n",
    "\n",
    "df_immigrations = create_immigrations()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+----------+----------+---------------+-----------------+----------------+------------------+----------------------+----------------------+----------+---------------------------------+------+\n",
      "|city           |state     |median_age|male_population|female_population|total_population|number_of_veterans|number_of_foreign_born|average_household_size|state_code|race                             |count |\n",
      "+---------------+----------+----------+---------------+-----------------+----------------+------------------+----------------------+----------------------+----------+---------------------------------+------+\n",
      "|Mesa           |Arizona   |36.9      |234998         |236835           |471833          |31808             |57492                 |2.68                  |AZ        |Hispanic or Latino               |131425|\n",
      "|Springdale     |Arkansas  |31.8      |36840          |43614            |80454           |3397              |19969                 |3.04                  |AR        |American Indian and Alaska Native|547   |\n",
      "|South Gate     |California|32.5      |47758          |48641            |96399           |724               |40571                 |3.97                  |CA        |Hispanic or Latino               |91312 |\n",
      "|Deerfield Beach|Florida   |41.4      |37155          |42614            |79769           |3882              |23642                 |2.46                  |FL        |Hispanic or Latino               |12729 |\n",
      "|Delray Beach   |Florida   |47.9      |32219          |34042            |66261           |4232              |16639                 |2.35                  |FL        |Asian                            |1696  |\n",
      "+---------------+----------+----------+---------------+-----------------+----------------+------------------+----------------------+----------------------+----------+---------------------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count and show 5 of the Demographics records.\n",
    "df_demographics_cl.show(5, truncate=False)\n",
    "df_demographics_cl.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------------------------------------------+------------+------------+--------+---------+----------+-----+------------------+---------------+\n",
      "|airport_id|type          |name                                              |elevation_ft|municipality|gps_code|iata_code|local_code|state|latitude          |longitude      |\n",
      "+----------+--------------+--------------------------------------------------+------------+------------+--------+---------+----------+-----+------------------+---------------+\n",
      "|KBGM      |medium_airport|Greater Binghamton/Edwin A Link field             |1636.0      |Binghamton  |KBGM    |BGM      |BGM       |NY   |-75.97979736      |42.20869827    |\n",
      "|2TE0      |small_airport |Eagle Air Park                                    |15.0        |Brazoria    |2TE0    |BZT      |2TE0      |TX   |-95.579696655273  |28.982200622559|\n",
      "|KCNU      |medium_airport|Chanute Martin Johnson Airport                    |1002.0      |Chanute     |KCNU    |CNU      |CNU       |KS   |-95.4850997925    |37.668800354   |\n",
      "|KCRS      |small_airport |C David Campbell Field Corsicana Municipal Airport|449.0       |Corsicana   |KCRS    |CRS      |CRS       |TX   |-96.4005966187    |32.0280990601  |\n",
      "|KFMY      |medium_airport|Page Field                                        |17.0        |Fort Myers  |KFMY    |FMY      |FMY       |FL   |-81.86329650879999|26.58659935    |\n",
      "+----------+--------------+--------------------------------------------------+------------+------------+--------+---------+----------+-----+------------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2014"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count and show 5 of the airport_codes records.\n",
    "\n",
    "df_spark_airport_codes_clean.show(5, truncate=False)\n",
    "df_spark_airport_codes_clean.count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------------------+-------+--------+---------+\n",
      "|date      |average_temperature|average_temperature_uncertainty|city   |latitude|longitude|\n",
      "+----------+-------------------+-------------------------------+-------+--------+---------+\n",
      "|1820-01-01|2.1010000000000004 |3.217                          |Abilene|32.95N  |100.53W  |\n",
      "|1820-02-01|6.926              |2.853                          |Abilene|32.95N  |100.53W  |\n",
      "|1820-03-01|10.767000000000001 |2.395                          |Abilene|32.95N  |100.53W  |\n",
      "|1820-04-01|17.988999999999994 |2.202                          |Abilene|32.95N  |100.53W  |\n",
      "|1820-05-01|21.809             |2.036                          |Abilene|32.95N  |100.53W  |\n",
      "+----------+-------------------+-------------------------------+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "661524"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count and show 5 of the temperature records. \n",
    "df_us_temperature.show(5, truncate=False)\n",
    "df_us_temperature.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+-----+-------------+-----------------+----+------------+------------+--------+--------------+----+--------------+----------+----------------------+----------+----------+------------+----------+-------+----------------+-------------+---------+\n",
      "|cicid    |year  |month|birth_country|residence_country|port|arrival_date|arrival_mode|us_state|departure_date|age |visa_type_code|date_added|visa_issued_department|occupation|birth_year|allowed_date|ins_number|airline|admission_number|flight_number|visa_type|\n",
      "+---------+------+-----+-------------+-----------------+----+------------+------------+--------+--------------+----+--------------+----------+----------------------+----------+----------+------------+----------+-------+----------------+-------------+---------+\n",
      "|1360029.0|2016.0|4.0  |116.0        |116.0            |ATL |2016-04-03  |Not reported|99      |2016-04-07    |55.0|Business      |20160408  |null                  |null      |1961.0    |null        |null      |null   |5.5527501433E10 |null         |WB       |\n",
      "|2046303.0|2016.0|4.0  |260.0        |260.0            |XXX |2016-04-08  |Not reported|99      |2016-04-11    |39.0|Business      |20160411  |null                  |null      |1977.0    |null        |null      |null   |7.20110585E8    |null         |B1       |\n",
      "|2289032.0|2016.0|4.0  |582.0        |582.0            |XXX |2016-04-11  |Not reported|99      |2016-04-12    |51.0|Business      |20160412  |null                  |null      |1965.0    |null        |null      |null   |9.009776333E10  |null         |B1       |\n",
      "|2462008.0|2016.0|4.0  |582.0        |582.0            |XXX |2016-04-12  |Not reported|99      |2016-04-13    |42.0|Business      |20160413  |null                  |null      |1974.0    |null        |null      |null   |6.203253393E10  |null         |E1       |\n",
      "|2645605.0|2016.0|4.0  |582.0        |582.0            |XXX |2016-04-10  |Not reported|99      |2016-04-14    |39.0|Business      |20160414  |null                  |null      |1977.0    |null        |null      |null   |9.323602493E10  |null         |B1       |\n",
      "+---------+------+-----+-------------+-----------------+----+------------+------------+--------+--------------+----+--------------+----------+----------------------+----------+----------+------------+----------+-------+----------------+-------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count and show 5 of the immigrations records. \n",
    "\n",
    "df_immigrations.show(5, truncate=False)\n",
    "df_immigrations.count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "The I94_SAS_Labels_Descriptions.SAS contains all the data field descriptions about the I94 immigration data. \n",
    "\n",
    "I94YR - 4 digit year\n",
    "\n",
    "I94MON - Numeric month\n",
    "\n",
    "I94CIT & I94RES - This format shows all the valid and invalid codes for processing\n",
    "\n",
    "I94PORT - This format shows all the valid and invalid codes for processing\n",
    "\n",
    "ARRDATE is the Arrival Date in the USA. It is a SAS date numeric field that a \n",
    "   permament format has not been applied.\n",
    "   \n",
    "I94MODE - There are missing values as well as not reported (9)\n",
    "\n",
    "I94ADDR - There is lots of invalid codes in this variable and the list below.\n",
    "\n",
    "DEPDATE is the Departure Date from the USA. It is a SAS date numeric field that \n",
    "   a permament format has not been applied.  \n",
    "\n",
    "   I94BIR - Age of Respondent in Years\n",
    "   \n",
    "   I94VISA - Visa codes collapsed into three categories:\n",
    "   1 = Business\n",
    "   2 = Pleasure\n",
    "   3 = Student\n",
    "   \n",
    "   COUNT - Used for summary statistics\n",
    "   \n",
    "   DTADFILE - Character Date Field - Date added to I-94 Files - CIC does not use\n",
    "   \n",
    "   VISAPOST - Department of State where where Visa was issued - CIC does not use\n",
    "   \n",
    "   OCCUP - Occupation that will be performed in U.S. - CIC does not use\n",
    "   \n",
    "   BIRYEAR - 4 digit year of birth\n",
    "   \n",
    "   DTADDTO - Character Date Field - Date to which admitted to U.S. (allowed to stay until) - CIC does not use\n",
    "   \n",
    "   GENDER - Non-immigrant sex\n",
    "   \n",
    "   INSNUM - INS number\n",
    "   \n",
    "   AIRLINE - Airline used to arrive in U.S.\n",
    "   \n",
    "   ADMNUM - Admission Number\n",
    "   \n",
    "   FLTNO - Flight number of Airline used to arrive in U.S.\n",
    "   \n",
    "   VISATYPE - Class of admission legally admitting the non-immigrant to temporarily stay in U.S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Export files \n",
    "Export the data tables into parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Export the demographics table.\n",
    "df_demographics_cl.write.mode(\"overwrite\").parquet(\"output/demographics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Export the airport codes table.\n",
    "df_spark_airport_codes_clean.write.mode(\"overwrite\").parquet(\"output/airport_codes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Export the temperature table partitioned by city field.\n",
    "df_us_temperature.write.mode(\"overwrite\").partitionBy(\"city\").parquet(\"output/temperatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Export the countries table.\n",
    "df_spark_countries.write.mode(\"overwrite\").parquet(\"output/countries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Export the immigrations table partitioned by year and month.\n",
    "df_immigrations.write.mode(\"overwrite\").partitionBy(\"year\",\"month\").parquet(\"output/immigrations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "\n",
    "I have worked using the Pandas and Spark dataframes depending on the volumes and the complexity of the data.\n",
    "In some cases, it's easier to use Pandas dataframes to perform cleaning / transformation of the data. When it comes to fast processing and Apache Spark dataframes is the best way to go. In addition, Spark gives the flexibility to handle different file formats (SAS) and perform standatd SQL queries on the tables (after creating Temporary Views on them).\n",
    "Defining the Schema is also a plus for the Apache Spark choice of working. The output files are parquet format, allowing us to partition and upload large datasets to a cloud solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Propose how often the data should be updated and why.\n",
    "\n",
    "The demographics data should be updated once a year. Immigration data, temperatures and airport codes should be updated once a month based on their lifecycle nature. In case they need to be updated more often, this should happen on a daily basis. Countries and other reference data can be updated rarely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    "#### The data was increased by 100x.\n",
    "If the project is heavy on reading over writing, I would store the data in AWS Redshift clusters, increasing the number of nodes and processing power.\n",
    "The solution is already implemented in Spark, so in the case of heavy writing I would use EMR (or a similar distributed cloud solution) to store the data into HDFS with Spark distribution.\n",
    "\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "The ETL steps can be implemented as DAG steps in Apache Airflow or a similar product. In that case we can choose which of the datasets should be updated daily and which of them less often.\n",
    "The DAG would be scheduled to run at 7am every day. In case of DAG failure, the Dashboard is not updated and a notification/email is sent to the affected teams/individuals.\n",
    "\n",
    "#### The database needed to be accessed by 100+ people\n",
    "A proven cloud solution like a DWH (Redshift) can be used so that the data can be consumed by a large number of people. However, considering the amount of queries and the availability in the day we can also run specific queries and store the results in S3 for quick access and consumption by the BI and Analytics apps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
